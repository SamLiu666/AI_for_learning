<!--
 * @Description: 
 * @Version: 2.0
 * @Autor: lxp
 * @Date: 2021-07-05 21:18:39
 * @LastEditors: lxp
 * @LastEditTime: 2021-07-17 21:22:48
-->
回归问题：

1、线性回归：

**线性** ： 两个变量之间的关系是一次函数关系，图像是直线叫做线性

**非线性**： 

过拟： L2、L1 正则化

损失函数：MSE

2、逻辑回归

思想：极大似然估计，将Y的结果带入非线性变换的Sigmoid函数中

损失函数： log loss 对数似然函数

3、决策树

ID3：取值多的属性，更容易使数据更纯，其信息增益更大。

训练得到的是一棵庞大且深度浅的树：不合理。

C4.5：采用信息增益率替代信息增益。

CART：以基尼系数替代熵，最小化不纯度，而不是最大化信息增益。

决策树的剪枝基本策略有 预剪枝 (Pre-Pruning) 和 后剪枝 (Post-Pruning)。

预剪枝：其中的核心思想就是，在每一次实际对结点进行进一步划分之前，先采用验证集的数据来验证如果划分是否能提高划分的准确性。如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成节点。

后剪枝：后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将该子树替换为叶结点

3.1 随机森林

Bagging是bootstrap aggregating。思想就是从总体样本当中随机取一部分样本进行训练，通过多次这样的结果，进行投票获取平均值作为结果输出，这就极大可能的避免了不好的样本数据，从而提高准确度。因为有些是不好的样本，相当于噪声，模型学入噪声后会使准确度不高。

Bagging策略来源于bootstrap aggregation：从样本集（假设样本集N个数据点）中重采样选出Nb个样本（有放回的采样，样本数据点个数仍然不变为N），在所有样本上，对这n个样本建立分类器（ID3\C4.5\CART\SVM\LOGISTIC），重复以上两步m次，获得m个分类器，最后根据这m个分类器的投票结果，决定数据属于哪一类。

每棵树的按照如下规则生成：

如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本，作为该树的训练集；
如果每个样本的特征维度为M，指定一个常数m<<M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；
每棵树都尽最大程度的生长，并且没有剪枝过程。
一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。

总的来说就是随机选择样本数，随机选取特征，随机选择分类器，建立多颗这样的决策树，然后通过这几课决策树来投票，决定数据属于哪一类(投票机制有一票否决制、少数服从多数、加权多数)

该模型容易过度拟合，因此，为了避免这些情况，我们要用交叉验证来调整树的数量

3.2 GBDT

GBDT(Gradient Boosting Decision Tree)，全名叫梯度提升决策树，使用的是Boosting的思想。

Boosting方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。

Bagging与Boosting的串行训练方式不同，Bagging方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。

3.2.1 优点
预测阶段的计算速度快，树与树之间可并行化计算。
在分布稠密的数据集上，泛化能力和表达能力都很好，这使得GBDT在Kaggle的众多竞赛中，经常名列榜首。
采用决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系。

3.2.2 局限性
GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。
GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。
训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。

3.3 XGBoost

除了算法上与传统的GBDT有一些不同外，XGBoost还在工程实现上做了大量的优化。总的来说，两者之间的区别和联系可以总结成以下几个方面。

GBDT是机器学习算法，XGBoost是该算法的工程实现。

在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模 型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。

GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代 价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。

传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类 器，比如线性分类器。

传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机 森林相似的策略，支持对数据进行采样。

传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺 失值的处理策略。

3.4 LightGBM
LightGBM （Light Gradient Boosting Machine）(请点击https://github.com/Microsoft/LightGBM)是一个实现GBDT算法的框架，支持高效率的并行训练，并且具有以下优点：

更快的训练速度
更低的内存消耗
更好的准确率
分布式支持，可以快速处理海量数据

4、 SVM 

超平面、几何间隔、最大间隔分类器、最大间隔损失函数Hinge loss

从线性可分转换为线性不可分--转换为对偶问题，是实质相同但从不同角度提出不同提法的一对问题。

核函数Kernel，将数据映射到高维空间，来解决在原始空间中线性不可分的问题。

如果数据中出现了离群点outliers，那么就可以使用松弛变量来解决

5、贝叶斯网络

