<!--
 * @Description: 
 * @Version: 2.0
 * @Autor: lxp
 * @Date: 2021-07-05 21:18:39
 * @LastEditors: lxp
 * @LastEditTime: 2021-07-18 15:58:27
-->
回归问题：

1、线性回归：

**线性** ： 两个变量之间的关系是一次函数关系，图像是直线叫做线性

**非线性**： 

过拟： L2、L1 正则化

损失函数：MSE

2、逻辑回归

思想：极大似然估计，将Y的结果带入非线性变换的Sigmoid函数中

损失函数： log loss 对数似然函数

3、决策树

ID3：取值多的属性，更容易使数据更纯，其信息增益更大。

训练得到的是一棵庞大且深度浅的树：不合理。

C4.5：采用信息增益率替代信息增益。

CART：以基尼系数替代熵，最小化不纯度，而不是最大化信息增益。

决策树的剪枝基本策略有 预剪枝 (Pre-Pruning) 和 后剪枝 (Post-Pruning)。

预剪枝：其中的核心思想就是，在每一次实际对结点进行进一步划分之前，先采用验证集的数据来验证如果划分是否能提高划分的准确性。如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成节点。

后剪枝：后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将该子树替换为叶结点

3.1 随机森林

Bagging是bootstrap aggregating。思想就是从总体样本当中随机取一部分样本进行训练，通过多次这样的结果，进行投票获取平均值作为结果输出，这就极大可能的避免了不好的样本数据，从而提高准确度。因为有些是不好的样本，相当于噪声，模型学入噪声后会使准确度不高。

Bagging策略来源于bootstrap aggregation：从样本集（假设样本集N个数据点）中重采样选出Nb个样本（有放回的采样，样本数据点个数仍然不变为N），在所有样本上，对这n个样本建立分类器（ID3\C4.5\CART\SVM\LOGISTIC），重复以上两步m次，获得m个分类器，最后根据这m个分类器的投票结果，决定数据属于哪一类。

每棵树的按照如下规则生成：

如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本，作为该树的训练集；
如果每个样本的特征维度为M，指定一个常数m<<M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；
每棵树都尽最大程度的生长，并且没有剪枝过程。
一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。

总的来说就是随机选择样本数，随机选取特征，随机选择分类器，建立多颗这样的决策树，然后通过这几课决策树来投票，决定数据属于哪一类(投票机制有一票否决制、少数服从多数、加权多数)

该模型容易过度拟合，因此，为了避免这些情况，我们要用交叉验证来调整树的数量

3.2 GBDT

GBDT(Gradient Boosting Decision Tree)，全名叫梯度提升决策树，使用的是Boosting的思想。

Boosting方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。

Bagging与Boosting的串行训练方式不同，Bagging方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。

3.2.1 优点
预测阶段的计算速度快，树与树之间可并行化计算。
在分布稠密的数据集上，泛化能力和表达能力都很好，这使得GBDT在Kaggle的众多竞赛中，经常名列榜首。
采用决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系。

3.2.2 局限性
GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。
GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。
训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。

3.3 XGBoost

除了算法上与传统的GBDT有一些不同外，XGBoost还在工程实现上做了大量的优化。总的来说，两者之间的区别和联系可以总结成以下几个方面。

GBDT是机器学习算法，XGBoost是该算法的工程实现。

在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模 型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。

GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代 价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。

传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类 器，比如线性分类器。

传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机 森林相似的策略，支持对数据进行采样。

传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺 失值的处理策略。

3.4 LightGBM
LightGBM （Light Gradient Boosting Machine）(请点击https://github.com/Microsoft/LightGBM)是一个实现GBDT算法的框架，支持高效率的并行训练，并且具有以下优点：

更快的训练速度
更低的内存消耗
更好的准确率
分布式支持，可以快速处理海量数据

4、 SVM 

超平面、几何间隔、最大间隔分类器、最大间隔损失函数Hinge loss

从线性可分转换为线性不可分--转换为对偶问题，是实质相同但从不同角度提出不同提法的一对问题。

核函数Kernel，将数据映射到高维空间，来解决在原始空间中线性不可分的问题。

如果数据中出现了离群点outliers，那么就可以使用松弛变量来解决

5、贝叶斯网络

概率图模型是用图来表示变量概率依赖关系的理论，结合概率论与图论的知识，利用图来表示与模型有关的变量的联合概率分布

有向边表示单向的依赖，无向边表示相互依赖关系。

球落在台球桌上的任一位置都有着相同的机会，即球落在台球桌上某一位置的概率服从均匀分布。这种在实验之前定下的属于基本前提性质的分布称为先验分布，或着无条件分布

后验分布π（θ|X）一般也认为是在给定样本X的情况下的θ条件分布，而使π（θ|X）达到最大的值θMD称为最大后验估计，类似于经典统计学中的极大似然估计

朴素贝叶斯朴素在哪里呢？ —— 两个假设：

一个特征出现的概率与其他特征（条件）独立；
每个特征同等重要。

判别模型(discriminative model)通过求解条件概率分布P(y|x)或者直接计算y的值来预测y。

线性回归（Linear Regression）,逻辑回归（Logistic Regression）,支持向量机（SVM）, 传统神经网络（Traditional Neural Networks）,线性判别分析（Linear Discriminative Analysis），条件随机场（Conditional Random Field）

生成模型（generative model）通过对观测值和标注数据计算联合概率分布P(x,y)来达到判定估算y的目的。

朴素贝叶斯（Naive Bayes）, 隐马尔科夫模型（HMM）,贝叶斯网络（Bayesian Networks）和隐含狄利克雷分布（Latent Dirichlet Allocation）、混合高斯模型

6、马尔可夫网络

马尔可夫模型（Markov Model）是一种统计模型，广泛应用在语音识别，词性自动标注，音字转换，概率文法等各个自然语言处理等应用领域。经过长期发展，尤其是在语音识别中的成功应用，使它成为一种通用的统计工具。到目前为止，它一直被认为是实现快速精确的语音识别系统的最成功的方法。

隐马尔可夫模型 (Hidden Markov Model) 是一种统计模型，用来描述一个含有隐含未知参数的马尔可夫过程。它是结构最简单的动态贝叶斯网，这是一种著名的有向图模型，主要用于时序数据建模，在语音识别、自然语言处理等领域有广泛应用。

7、主题模型

将文档集 中每篇文档的主题以概率分布的形式给出，从而通过分析一些文档抽取出它们的主题（分布）出来后，便可以根据主题（分布）进行主题聚类或文本分类。同时，它是一种典型的词袋模型，即一篇文档是由一组词构成，词与词之间没有先后顺序的关系。此外，一篇文档可以包含多个主题，文档中每一个词都由其中的一个主题生成。

LDA就是跟这个反过来：根据给定的一篇文档，反推其主题分布。

8、最大期望算法

最大期望算法（Expectation-maximization algorithm，又译为期望最大化算法），是在概率模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐性变量

在数理统计学中，似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性。而极大似然就相当于最大可能的意思。

多数情况下我们是根据已知条件来推算结果，而最大似然估计是已经知道了结果，然后寻求使该结果出现的可能性最大的条件，以此作为估计值。

9、聚类算法

k-means(k均值)算法

10、KNN算法

距离算法：欧氏距离、曼哈顿距离、切比雪夫距离、余弦距离、皮尔逊系数